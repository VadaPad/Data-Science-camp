{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Text classification: topic modeling \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Latent Dirichlet allocation (LDA)\n",
    "\n",
    "</font>\n",
    "\n",
    "Typically used to detect underlying topics in the text documents\n",
    "\n",
    "**Input** : text documents and number of topics \n",
    "<br>\n",
    "**Output**: Distribution of topics for each document (that allows to assign the one with highest probability) and word distribution for each topic \n",
    "\n",
    "**Assumptions**:\n",
    "- Documents with similar topics use similar groups of words \n",
    "- Documents are probability distribution over latent topics \n",
    "- Topics are probability distribution over words\n",
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "### Generative process\n",
    "\n",
    "</font>\n",
    "\n",
    "LDA considers the every document is created the following way:\n",
    "\n",
    "1) Define number of words in the document\n",
    "<br>\n",
    "2) Chose the topic mixture over the fixed set of topics (e.g. 20% of topic 'Financial', 30% of topic 'Computer Science', and 50% of topic 'Sport')\n",
    "<br>\n",
    "3) Generate the words by:\n",
    "<br>\n",
    "   -pick the topic based on document's multinomial distribution \n",
    "<br>\n",
    "   -pick the word based on topic's multinomial distribution \n",
    "\n",
    "<img src = \"topics_modeling2.jpg\" height=500 width= 800 align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Recall\n",
    "</font>\n",
    "\n",
    "\n",
    "#### Binomial distribution\n",
    "\n",
    "$$p(k/n)\\quad =\\quad C^{ k }_{ n }\\cdot p^{ k }(1-p)^{ n-k }\\quad =\\quad \\frac { n! }{ k!(n-k)! } p^{ k }(1-p)^{ n-k }$$\n",
    "\n",
    "Example: Probability of 6 of 10 for fear coin: \n",
    "$$p(6,4)\\quad =\\quad C^{ 6 }_{ 10 }\\cdot {0.5}^{ 6 }(0.5)^{ 4 }\\quad = 210 \\cdot 0.015625 \\cdot 0.0625 = 0.205078125$$\n",
    "\n",
    "\n",
    "#### Multinomial distribution\n",
    "\n",
    "$$p(n_{ 1 }n_{ 2 }...n_{ k })\\quad =\\quad \\frac { n! }{ n_{ 1 }!n_{ 2 }!...n_{ k }! } p^{ n_{ 1 } }_{ 1 }p^{ n_{ 2 } }_{ 2 }...p^{ n_{ k } }_{ k }$$\n",
    "\n",
    "Example (three outcomes): <br>\n",
    "n = 12 (12 games are played),<br>\n",
    "n1 = 7 (number won by Player A),<br>\n",
    "n2 = 2 (number won by Player B),<br>\n",
    "n3 = 3 (the number drawn),<br>\n",
    "p1 = 0.4 (probability Player A wins)<br>\n",
    "p2 = 0.35(probability Player B wins)<br>\n",
    "p3 = 0.25(probability of a draw)<br>\n",
    "$$p(7,2,3)\\quad =\\quad \\frac {12!}{ 7! \\cdot 2! \\cdot3 ! }  \\cdot 0.4^{7} \\cdot 0.35^{2} \\cdot0.25^{3} = 0.0248$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Maximul Likelihood Estimation\n",
    "    \n",
    "#### Simple sample\n",
    "    \n",
    "</font>\n",
    "\n",
    "Data is factully sampled `Head Tail Head` (101)\n",
    "\n",
    "Let's investigate  parametr `p` the probability of flipping `Head`\n",
    "\n",
    "<!-- <img src = \"MLE.jpg\" height=500 width= 500 align=\"left\"> -->\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.05 0.1  0.15 0.2  0.25 0.3  0.35 0.4  0.45 0.5  0.55 0.6  0.65\n",
      " 0.7  0.75 0.8  0.85 0.9  0.95 1.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.68, 0.156, 'MLE')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot  as plt \n",
    "p = np.linspace(0,1,21) \n",
    "print (p )\n",
    "data_prob = p**2 * (1-p) \n",
    "plt.scatter (p,  data_prob) \n",
    "plt.xlabel('p ')\n",
    "plt.ylabel('p (data=HTH)')\n",
    "plt.axvline(x=2/3, color='r')\n",
    "plt.text(.68, .156 ,'MLE', fontsize= 16, color = 'r', bbox=dict(facecolor='none', edgecolor='gray', pad=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have documents (instead of coin flips) and need to find the distributions (instead of `p` for coin sample) s.t. is MLE for data (all documents)\n",
    "\n",
    "**Recall** \n",
    "<br> Known are text documents and number $K$ of topics \n",
    "\n",
    "**Target**:\n",
    "<br>Within all possible topics distribution for all documemnts and all possible words distribution for topics, shoose the one wich maximizes probability of all text documents.\n",
    "\n",
    "**Note:** It is unclear how to iterate over all possible distributions \n",
    "\n",
    "**Approach** :\n",
    "<br>\n",
    "1) Randomly assign each word of each document to $K$ topics \n",
    "<br>\n",
    "2) Iterate the following process till convergence (steady assignments of w to topics) \n",
    "<br>$\\quad$>For each document $d$: \n",
    "<br>\n",
    "    $\\quad\\quad\\bullet$ Assume that all topic assignment except current one are correct     \n",
    "    $\\quad\\quad\\bullet$ For each word $w$ in $d$:           \n",
    "    $\\quad\\quad\\quad$ - For every topic $t$ compare the the score for hypothesis that w is in this topic $t$:\n",
    "   <br>$\\quad\\quad\\quad\\quad\\quad score (t) =  p(t | d) \\cdot p (w |t),$\n",
    "   <br>$\\quad\\quad\\quad\\quad p(t|d)$ is proportion of all words in d from t,\n",
    "    <br>$\\quad\\quad\\quad\\quad p(w|t)$ is share of word w in topic t.  \n",
    "    $\\quad\\quad\\quad$ - Assign the word w to the topic with max score\n",
    "    <br>$\\quad\\quad\\bullet$ Iterate through all $w$ in $d$:           \n",
    "$\\quad$Iterate through all $d$\n",
    "\n",
    "The results is matrix of distribution of words in topics  \n",
    "\n",
    "Note: \n",
    "- The computed topics are just words distribution, i.e. need to summarize them somehow \n",
    "- Topics distribution over documents are computed being based on words in document and corresponding topics of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.0-py3-none-any.whl (58 kB)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\vsams\\anaconda\\lib\\site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\vsams\\anaconda\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1 smart-open-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Gensim LDA \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vsams\\ANACONDA\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Define the text documents \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Tokenize, clean, and stem\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brocolli',\n",
       " 'good',\n",
       " 'eat',\n",
       " '.',\n",
       " 'brother',\n",
       " 'like',\n",
       " 'eat',\n",
       " 'good',\n",
       " 'brocolli',\n",
       " ',',\n",
       " 'mother',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stop  = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(doc_set):\n",
    "    texts = []\n",
    "    for doc in doc_set:\n",
    "        # tokenize document string\n",
    "        raw = doc.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        tokens = [p_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(tokens)\n",
    "    return texts\n",
    "\n",
    "texts = tokenize(doc_set)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Convert tokenized documents into a \"id <-> term\" dictionary\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'> Dictionary(34 unique tokens: [',', '.', 'brocolli', 'brother', 'eat']...)\n",
      "0 ,\n",
      "1 .\n",
      "2 brocolli\n",
      "3 brother\n",
      "4 eat\n",
      "5 good\n",
      "6 like\n",
      "7 mother\n",
      "8 around\n",
      "9 basebal\n",
      "10 drive\n",
      "11 lot\n",
      "12 practic\n",
      "13 spend\n",
      "14 time\n",
      "15 blood\n",
      "16 caus\n",
      "17 expert\n",
      "18 health\n",
      "19 increas\n",
      "20 may\n",
      "21 pressur\n",
      "22 suggest\n",
      "23 tension\n",
      "24 better\n",
      "25 feel\n",
      "26 never\n",
      "27 often\n",
      "28 perform\n",
      "29 school\n",
      "30 seem\n",
      "31 well\n",
      "32 profession\n",
      "33 say\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts) # this is alternative way - without using count vectorizer\n",
    "print (type(dictionary), dictionary)\n",
    "for k,w in dictionary.items():\n",
    "    print (k,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Create gensim corpus\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "convert tokenized documents into a document-term matrix\n",
      "[(0, 1), (1, 2), (2, 2), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1)]\n",
      "[(1, 1), (3, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(1, 1), (10, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]\n",
      "[(0, 1), (1, 1), (3, 1), (7, 1), (10, 1), (21, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(1, 1), (2, 1), (5, 1), (18, 2), (32, 1), (33, 1)]\n"
     ]
    }
   ],
   "source": [
    "print ('\\nconvert tokenized documents into a document-term matrix')\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # id and count\n",
    "for item in corpus:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "It shows the id of term and how many tiumes it occurs in the doc e.g. \n",
    "- (3, 1) means `brother` occurs once  in the second sentence\n",
    "- (18, 2) means `health` occurs twice in the last sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Generate LDA model\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=2, id2word=dictionary, passes=20, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Review topics \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.098*\".\" + 0.076*\"brocolli\" + 0.076*\"good\" + 0.055*\"mother\" + 0.055*\"brother\" + 0.054*\"health\" + 0.054*\"eat\" + 0.033*\",\" + 0.033*\"like\" + 0.033*\"spend\" + 0.033*\"time\" + 0.033*\"around\" + 0.033*\"basebal\" + 0.033*\"lot\" + 0.033*\"practic\" + 0.033*\"say\" + 0.033*\"profession\" + 0.032*\"drive\" + 0.011*\"pressur\" + 0.011*\"caus\" + 0.011*\"may\" + 0.011*\"suggest\" + 0.011*\"expert\" + 0.011*\"feel\" + 0.011*\"blood\" + 0.011*\"better\" + 0.011*\"school\" + 0.011*\"tension\" + 0.011*\"well\" + 0.011*\"seem\" + 0.011*\"perform\" + 0.011*\"increas\" + 0.011*\"often\" + 0.011*\"never\"'),\n",
       " (1,\n",
       "  '0.060*\"drive\" + 0.059*\"pressur\" + 0.059*\".\" + 0.036*\",\" + 0.036*\"never\" + 0.036*\"often\" + 0.036*\"increas\" + 0.036*\"perform\" + 0.036*\"seem\" + 0.036*\"well\" + 0.036*\"tension\" + 0.036*\"school\" + 0.036*\"better\" + 0.036*\"blood\" + 0.036*\"feel\" + 0.036*\"expert\" + 0.036*\"suggest\" + 0.036*\"may\" + 0.036*\"caus\" + 0.036*\"health\" + 0.035*\"brother\" + 0.035*\"mother\" + 0.012*\"profession\" + 0.012*\"say\" + 0.012*\"practic\" + 0.012*\"lot\" + 0.012*\"basebal\" + 0.012*\"around\" + 0.012*\"time\" + 0.012*\"spend\" + 0.012*\"good\" + 0.012*\"brocolli\" + 0.012*\"eat\" + 0.012*\"like\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=2,num_words=60) # 0.098 means the p (w|t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Classify the new text \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get topics:\n",
      "[(0, 0.3741765), (1, 0.62582344)]\n"
     ]
    }
   ],
   "source": [
    "test_doc_list = [\"Some experts suggest that car may cause increased blood pressure. professionals say that brocolli is good \"]\n",
    "test_texts = tokenize(test_doc_list)\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_texts ]\n",
    "test_doc_topics = ldamodel.get_document_topics(test_corpus)\n",
    "print ('\\nget topics:')\n",
    "for el in test_doc_topics: # loop over all tests in provided list\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Sample of topic modeling on large dataset\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load \"News\" data \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd()\n",
    "fn=  'newsgroups'\n",
    "\n",
    "with open(fn, 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review data\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "len of documents = 2,000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The best group to keep you informed is the Crohn's and Colitis Foundation\\nof America.  I do not know if the UK has a similar organization.  The\\naddress of\\nthe CCFA is \\n\\nCCFA\\n444 Park Avenue South\\n11th Floor\\nNew York, NY  10016-7374\\nUSA\\n\\nThey have a lot of information available and have a number of newsletters.\\n \\nGood Luck.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (type(newsgroup_data))\n",
    "print ('len of documents = {:,}\\n'.format(len(newsgroup_data)))\n",
    "\n",
    "newsgroup_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Define custom vectorizer\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=20, stop_words='english', token_pattern='\\\\b\\\\w{3,}\\\\b')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_words_pattern = r\"\\b\\w{3,}\\b\"\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=20, \n",
    "    stop_words='english',\n",
    "    token_pattern=three_words_pattern) \n",
    "vectorizer.fit(newsgroup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review feratures \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features = 902\n",
      "\n",
      "['000', '100', '1990', '1992', '1993', '200', '2nd', '300', '400', '486', '500', '800', 'ability', 'able', 'accept', 'accepted', 'access', 'according', 'actual', 'actually', 'add', 'addition', 'additional', 'address', 'advance', 'advice', 'age', 'ago', 'agree', 'ahead', 'air', 'allow', 'alt', 'america', 'american', 'answer', 'answers', 'anybody', 'apparently', 'appears']\n"
     ]
    }
   ],
   "source": [
    "print ('len of features = {:,}\\n'.format(len(vectorizer.get_feature_names())))\n",
    "print (vectorizer.get_feature_names()[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Vectorize data set\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 326)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 386)\t1\n",
      "  (0, 409)\t1\n",
      "  (0, 451)\t1\n",
      "  (0, 456)\t1\n",
      "  (0, 515)\t1\n",
      "  (0, 529)\t1\n",
      "  (0, 545)\t1\n",
      "  (0, 727)\t1\n",
      "  (0, 843)\t1\n",
      "  (0, 900)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 84)\t1\n",
      "  (1, 184)\t1\n",
      "  (1, 201)\t1\n",
      "  (1, 214)\t1\n",
      "  (1, 231)\t2\n",
      "  (1, 241)\t1\n",
      "  (1, 324)\t1\n",
      "  :\t:\n",
      "  (1998, 622)\t1\n",
      "  (1998, 625)\t3\n",
      "  (1998, 688)\t1\n",
      "  (1998, 698)\t2\n",
      "  (1998, 726)\t1\n",
      "  (1998, 804)\t1\n",
      "  (1998, 805)\t1\n",
      "  (1998, 810)\t10\n",
      "  (1998, 813)\t2\n",
      "  (1998, 814)\t1\n",
      "  (1998, 816)\t1\n",
      "  (1998, 818)\t1\n",
      "  (1998, 844)\t1\n",
      "  (1998, 882)\t2\n",
      "  (1998, 899)\t1\n",
      "  (1999, 171)\t1\n",
      "  (1999, 194)\t1\n",
      "  (1999, 205)\t1\n",
      "  (1999, 213)\t1\n",
      "  (1999, 276)\t2\n",
      "  (1999, 308)\t1\n",
      "  (1999, 344)\t1\n",
      "  (1999, 669)\t1\n",
      "  (1999, 832)\t1\n",
      "  (1999, 874)\t1\n"
     ]
    }
   ],
   "source": [
    "newsgroup_data_vectorized= vectorizer.transform(newsgroup_data)\n",
    "print (newsgroup_data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Create gensim corpus\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(23, 1),\n",
       "  (33, 1),\n",
       "  (58, 1),\n",
       "  (76, 1),\n",
       "  (326, 1),\n",
       "  (335, 1),\n",
       "  (386, 1),\n",
       "  (409, 1),\n",
       "  (451, 1),\n",
       "  (456, 1),\n",
       "  (515, 1),\n",
       "  (529, 1),\n",
       "  (545, 1),\n",
       "  (727, 1),\n",
       "  (843, 1),\n",
       "  (900, 1)],\n",
       " [(33, 1),\n",
       "  (34, 1),\n",
       "  (84, 1),\n",
       "  (184, 1),\n",
       "  (201, 1),\n",
       "  (214, 1),\n",
       "  (231, 2),\n",
       "  (241, 1),\n",
       "  (324, 1),\n",
       "  (332, 1),\n",
       "  (359, 1),\n",
       "  (363, 1),\n",
       "  (365, 1),\n",
       "  (409, 1),\n",
       "  (430, 3),\n",
       "  (451, 1),\n",
       "  (475, 1),\n",
       "  (492, 2),\n",
       "  (525, 2),\n",
       "  (605, 1),\n",
       "  (633, 2),\n",
       "  (642, 1),\n",
       "  (674, 1),\n",
       "  (688, 1),\n",
       "  (709, 1),\n",
       "  (750, 1),\n",
       "  (777, 1),\n",
       "  (823, 1),\n",
       "  (838, 1),\n",
       "  (874, 1),\n",
       "  (896, 1)],\n",
       " [(25, 1),\n",
       "  (26, 1),\n",
       "  (63, 1),\n",
       "  (120, 1),\n",
       "  (231, 1),\n",
       "  (297, 1),\n",
       "  (326, 1),\n",
       "  (344, 1),\n",
       "  (373, 1),\n",
       "  (423, 1),\n",
       "  (442, 1),\n",
       "  (444, 1),\n",
       "  (448, 2),\n",
       "  (465, 1),\n",
       "  (572, 1),\n",
       "  (653, 1),\n",
       "  (659, 1),\n",
       "  (714, 1),\n",
       "  (777, 1),\n",
       "  (779, 1),\n",
       "  (781, 1),\n",
       "  (818, 1),\n",
       "  (836, 1),\n",
       "  (855, 1),\n",
       "  (890, 1),\n",
       "  (898, 1)],\n",
       " [(4, 1),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (22, 1),\n",
       "  (42, 1),\n",
       "  (48, 2),\n",
       "  (68, 1),\n",
       "  (78, 1),\n",
       "  (86, 1),\n",
       "  (94, 1),\n",
       "  (117, 1),\n",
       "  (119, 1),\n",
       "  (122, 1),\n",
       "  (148, 1),\n",
       "  (155, 1),\n",
       "  (169, 1),\n",
       "  (210, 2),\n",
       "  (232, 1),\n",
       "  (242, 7),\n",
       "  (262, 1),\n",
       "  (297, 1),\n",
       "  (348, 2),\n",
       "  (360, 3),\n",
       "  (367, 2),\n",
       "  (374, 1),\n",
       "  (378, 2),\n",
       "  (384, 1),\n",
       "  (386, 4),\n",
       "  (392, 1),\n",
       "  (397, 2),\n",
       "  (403, 1),\n",
       "  (423, 1),\n",
       "  (426, 1),\n",
       "  (438, 1),\n",
       "  (440, 3),\n",
       "  (462, 1),\n",
       "  (466, 1),\n",
       "  (470, 2),\n",
       "  (471, 1),\n",
       "  (483, 1),\n",
       "  (486, 2),\n",
       "  (506, 1),\n",
       "  (509, 1),\n",
       "  (510, 2),\n",
       "  (515, 3),\n",
       "  (553, 2),\n",
       "  (558, 2),\n",
       "  (572, 1),\n",
       "  (588, 2),\n",
       "  (612, 5),\n",
       "  (615, 1),\n",
       "  (616, 1),\n",
       "  (635, 1),\n",
       "  (641, 1),\n",
       "  (646, 1),\n",
       "  (652, 1),\n",
       "  (655, 1),\n",
       "  (688, 2),\n",
       "  (695, 3),\n",
       "  (715, 3),\n",
       "  (727, 1),\n",
       "  (746, 1),\n",
       "  (762, 1),\n",
       "  (765, 1),\n",
       "  (768, 1),\n",
       "  (780, 1),\n",
       "  (793, 1),\n",
       "  (798, 2),\n",
       "  (799, 3),\n",
       "  (818, 1),\n",
       "  (844, 4),\n",
       "  (894, 1),\n",
       "  (901, 2)],\n",
       " [(334, 3), (466, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "# comparing to using corpora.Dictionary:\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts] \n",
    "[item for item in corpus][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Create id2word dictionary\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{76: 'best',\n",
       " 335: 'group',\n",
       " 33: 'america',\n",
       " 409: 'know',\n",
       " 727: 'similar',\n",
       " 545: 'organization',\n",
       " 23: 'address',\n",
       " 515: 'new',\n",
       " 900: 'york',\n",
       " 843: 'usa',\n",
       " 451: 'lot',\n",
       " 386: 'information',\n",
       " 58: 'available',\n",
       " 529: 'number',\n",
       " 326: 'good',\n",
       " 456: 'luck',\n",
       " 750: 'sounds',\n",
       " 430: 'like',\n",
       " 84: 'blood',\n",
       " 359: 'hey',\n",
       " 709: 'send',\n",
       " 231: 'don',\n",
       " 777: 'study',\n",
       " 363: 'history',\n",
       " 525: 'north',\n",
       " 241: 'early',\n",
       " 605: 'probably',\n",
       " 34: 'american',\n",
       " 492: 'mode',\n",
       " 838: 'understand',\n",
       " 201: 'decide',\n",
       " 633: 'reading',\n",
       " 475: 'mean',\n",
       " 688: 'said',\n",
       " 823: 'toronto',\n",
       " 324: 'going',\n",
       " 874: 'win',\n",
       " 184: 'cup',\n",
       " 896: 'yeah',\n",
       " 674: 'right',\n",
       " 642: 'rec',\n",
       " 365: 'hockey',\n",
       " 332: 'great',\n",
       " 214: 'didn',\n",
       " 779: 'stupid',\n",
       " 423: 'leave',\n",
       " 572: 'place',\n",
       " 63: 'bad',\n",
       " 25: 'advice',\n",
       " 465: 'major',\n",
       " 344: 'hard',\n",
       " 448: 'lose',\n",
       " 818: 'time',\n",
       " 442: 'long',\n",
       " 26: 'age',\n",
       " 373: 'hurt',\n",
       " 898: 'years',\n",
       " 297: 'following',\n",
       " 855: 'video',\n",
       " 836: 'type',\n",
       " 120: 'caused',\n",
       " 653: 'relatively',\n",
       " 890: 'worst',\n",
       " 714: 'seriously',\n",
       " 781: 'suggest',\n",
       " 444: 'look',\n",
       " 659: 'replace',\n",
       " 641: 'reasons',\n",
       " 655: 'religious',\n",
       " 148: 'com',\n",
       " 780: 'subject',\n",
       " 4: '1993',\n",
       " 242: 'earth',\n",
       " 715: 'service',\n",
       " 646: 'recently',\n",
       " 17: 'according',\n",
       " 262: 'exists',\n",
       " 652: 'related',\n",
       " 42: 'applications',\n",
       " 471: 'mark',\n",
       " 765: 'statement',\n",
       " 901: 'young',\n",
       " 558: 'people',\n",
       " 510: 'need',\n",
       " 78: 'better',\n",
       " 155: 'common',\n",
       " 119: 'cause',\n",
       " 844: 'use',\n",
       " 438: 'live',\n",
       " 588: 'possible',\n",
       " 348: 'having',\n",
       " 48: 'areas',\n",
       " 615: 'provide',\n",
       " 612: 'program',\n",
       " 384: 'including',\n",
       " 894: 'written',\n",
       " 378: 'ideas',\n",
       " 509: 'necessary',\n",
       " 486: 'middle',\n",
       " 360: 'high',\n",
       " 695: 'school',\n",
       " 799: 'teams',\n",
       " 440: 'local',\n",
       " 18: 'actual',\n",
       " 798: 'team',\n",
       " 768: 'status',\n",
       " 68: 'based',\n",
       " 122: 'certain',\n",
       " 22: 'additional',\n",
       " 746: 'soon',\n",
       " 553: 'particularly',\n",
       " 397: 'involved',\n",
       " 506: 'national',\n",
       " 426: 'level',\n",
       " 210: 'development',\n",
       " 470: 'manual',\n",
       " 793: 'takes',\n",
       " 466: 'make',\n",
       " 616: 'public',\n",
       " 367: 'home',\n",
       " 483: 'message',\n",
       " 86: 'board',\n",
       " 117: 'case',\n",
       " 635: 'real',\n",
       " 462: 'mail',\n",
       " 94: 'box',\n",
       " 392: 'interested',\n",
       " 762: 'starting',\n",
       " 403: 'just',\n",
       " 169: 'contact',\n",
       " 374: 'ibm',\n",
       " 232: 'dos',\n",
       " 334: 'ground',\n",
       " 607: 'problems',\n",
       " 716: 'set',\n",
       " 828: 'transfer',\n",
       " 47: 'area',\n",
       " 172: 'controller',\n",
       " 864: 'way',\n",
       " 606: 'problem',\n",
       " 10: '500',\n",
       " 590: 'post',\n",
       " 566: 'picture',\n",
       " 281: 'fault',\n",
       " 390: 'instead',\n",
       " 317: 'getting',\n",
       " 192: 'dave',\n",
       " 848: 'using',\n",
       " 783: 'sun',\n",
       " 347: 'haven',\n",
       " 711: 'sent',\n",
       " 731: 'single',\n",
       " 337: 'guy',\n",
       " 447: 'looks',\n",
       " 352: 'heard',\n",
       " 167: 'considered',\n",
       " 310: 'gas',\n",
       " 473: 'matter',\n",
       " 144: 'clean',\n",
       " 887: 'works',\n",
       " 116: 'cars',\n",
       " 217: 'different',\n",
       " 55: 'assume',\n",
       " 425: 'let',\n",
       " 351: 'hear',\n",
       " 541: 'opinions',\n",
       " 813: 'think',\n",
       " 270: 'face',\n",
       " 625: 'quite',\n",
       " 194: 'day',\n",
       " 599: 'pretty',\n",
       " 350: 'health',\n",
       " 53: 'ask',\n",
       " 764: 'state',\n",
       " 879: 'won',\n",
       " 399: 'issue',\n",
       " 594: 'power',\n",
       " 756: 'speed',\n",
       " 98: 'btw',\n",
       " 549: 'owner',\n",
       " 57: 'auto',\n",
       " 185: 'current',\n",
       " 111: 'car',\n",
       " 138: 'chris',\n",
       " 469: 'man',\n",
       " 476: 'means',\n",
       " 401: 'job',\n",
       " 446: 'looking',\n",
       " 751: 'source',\n",
       " 421: 'league',\n",
       " 67: 'baseball',\n",
       " 578: 'players',\n",
       " 858: 'want',\n",
       " 436: 'list',\n",
       " 519: 'nice',\n",
       " 663: 'reports',\n",
       " 866: 'week',\n",
       " 228: 'does',\n",
       " 377: 'idea',\n",
       " 175: 'cost',\n",
       " 809: 'thanks',\n",
       " 546: 'original',\n",
       " 621: 'question',\n",
       " 288: 'final',\n",
       " 213: 'did',\n",
       " 477: 'medical',\n",
       " 114: 'care',\n",
       " 651: 'regular',\n",
       " 95: 'break',\n",
       " 356: 'help',\n",
       " 839: 'unfortunately',\n",
       " 857: 'wait',\n",
       " 859: 'wanted',\n",
       " 170: 'continue',\n",
       " 782: 'suggestions',\n",
       " 899: 'yes',\n",
       " 150: 'comes',\n",
       " 30: 'air',\n",
       " 286: 'figure',\n",
       " 75: 'believe',\n",
       " 364: 'hit',\n",
       " 59: 'average',\n",
       " 804: 'terms',\n",
       " 74: 'begin',\n",
       " 488: 'miles',\n",
       " 862: 'wasn',\n",
       " 207: 'designed',\n",
       " 279: 'fast',\n",
       " 523: 'normal',\n",
       " 88: 'body',\n",
       " 206: 'design',\n",
       " 676: 'road',\n",
       " 0: '000',\n",
       " 234: 'drive',\n",
       " 181: 'course',\n",
       " 453: 'love',\n",
       " 79: 'big',\n",
       " 489: 'mind',\n",
       " 393: 'interesting',\n",
       " 452: 'lots',\n",
       " 870: 'west',\n",
       " 753: 'space',\n",
       " 191: 'date',\n",
       " 277: 'faq',\n",
       " 705: 'section',\n",
       " 591: 'posted',\n",
       " 696: 'sci',\n",
       " 852: 'various',\n",
       " 291: 'fine',\n",
       " 406: 'kill',\n",
       " 666: 'requires',\n",
       " 64: 'ball',\n",
       " 314: 'generally',\n",
       " 637: 'really',\n",
       " 778: 'stuff',\n",
       " 300: 'forget',\n",
       " 316: 'gets',\n",
       " 507: 'natural',\n",
       " 544: 'order',\n",
       " 512: 'needs',\n",
       " 480: 'men',\n",
       " 243: 'easily',\n",
       " 846: 'useful',\n",
       " 496: 'money',\n",
       " 508: 'near',\n",
       " 402: 'john',\n",
       " 467: 'makes',\n",
       " 562: 'person',\n",
       " 561: 'period',\n",
       " 648: 'red',\n",
       " 85: 'blue',\n",
       " 311: 'gave',\n",
       " 632: 'read',\n",
       " 89: 'book',\n",
       " 263: 'expect',\n",
       " 322: 'god',\n",
       " 246: 'eat',\n",
       " 802: 'tell',\n",
       " 526: 'note',\n",
       " 182: 'cover',\n",
       " 622: 'questions',\n",
       " 118: 'cases',\n",
       " 1: '100',\n",
       " 725: 'signal',\n",
       " 208: 'details',\n",
       " 82: 'bit',\n",
       " 596: 'pre',\n",
       " 583: 'point',\n",
       " 555: 'pass',\n",
       " 736: 'slightly',\n",
       " 524: 'normally',\n",
       " 380: 'important',\n",
       " 258: 'exactly',\n",
       " 760: 'start',\n",
       " 800: 'technical',\n",
       " 692: 'say',\n",
       " 815: 'thought',\n",
       " 787: 'sure',\n",
       " 328: 'got',\n",
       " 734: 'size',\n",
       " 576: 'played',\n",
       " 12: 'ability',\n",
       " 897: 'year',\n",
       " 829: 'tried',\n",
       " 575: 'play',\n",
       " 19: 'actually',\n",
       " 216: 'difference',\n",
       " 349: 'head',\n",
       " 354: 'hell',\n",
       " 758: 'sports',\n",
       " 707: 'seen',\n",
       " 266: 'experience',\n",
       " 394: 'interface',\n",
       " 845: 'used',\n",
       " 741: 'software',\n",
       " 717: 'setting',\n",
       " 275: 'fan',\n",
       " 37: 'anybody',\n",
       " 518: 'nhl',\n",
       " 130: 'cheap',\n",
       " 720: 'shot',\n",
       " 308: 'game',\n",
       " 463: 'main',\n",
       " 618: 'purpose',\n",
       " 772: 'stop',\n",
       " 604: 'pro',\n",
       " 771: 'stick',\n",
       " 124: 'chance',\n",
       " 571: 'pittsburgh',\n",
       " 307: 'future',\n",
       " 77: 'bet',\n",
       " 891: 'worth',\n",
       " 577: 'player',\n",
       " 657: 'remove',\n",
       " 527: 'notice',\n",
       " 325: 'gone',\n",
       " 164: 'connection',\n",
       " 881: 'wondering',\n",
       " 412: 'knows',\n",
       " 171: 'control',\n",
       " 884: 'work',\n",
       " 834: 'turn',\n",
       " 434: 'line',\n",
       " 694: 'says',\n",
       " 428: 'life',\n",
       " 159: 'complete',\n",
       " 608: 'process',\n",
       " 303: 'free',\n",
       " 215: 'die',\n",
       " 690: 'save',\n",
       " 851: 'value',\n",
       " 199: 'death',\n",
       " 255: 'especially',\n",
       " 712: 'serial',\n",
       " 160: 'completely',\n",
       " 13: 'able',\n",
       " 330: 'government',\n",
       " 639: 'reason',\n",
       " 196: 'dead',\n",
       " 418: 'law',\n",
       " 381: 'include',\n",
       " 681: 'rule',\n",
       " 682: 'rules',\n",
       " 895: 'wrong',\n",
       " 747: 'sorry',\n",
       " 197: 'deal',\n",
       " 740: 'society',\n",
       " 691: 'saw',\n",
       " 389: 'installed',\n",
       " 198: 'dealer',\n",
       " 251: 'email',\n",
       " 450: 'lost',\n",
       " 256: 'european',\n",
       " 742: 'sold',\n",
       " 179: 'country',\n",
       " 341: 'happen',\n",
       " 814: 'thinking',\n",
       " 863: 'watch',\n",
       " 103: 'buy',\n",
       " 611: 'products',\n",
       " 276: 'fans',\n",
       " 145: 'clear',\n",
       " 49: 'aren',\n",
       " 340: 'hand',\n",
       " 3: '1992',\n",
       " 867: 'weeks',\n",
       " 27: 'ago',\n",
       " 656: 'remember',\n",
       " 93: 'bought',\n",
       " 195: 'days',\n",
       " 822: 'took',\n",
       " 83: 'black',\n",
       " 669: 'rest',\n",
       " 429: 'light',\n",
       " 552: 'particular',\n",
       " 745: 'somewhat',\n",
       " 336: 'guess',\n",
       " 610: 'product',\n",
       " 472: 'market',\n",
       " 743: 'solution',\n",
       " 333: 'greatly',\n",
       " 44: 'appreciated',\n",
       " 50: 'argument',\n",
       " 774: 'straight',\n",
       " 854: 'version',\n",
       " 849: 'usual',\n",
       " 810: 'theory',\n",
       " 892: 'wouldn',\n",
       " 811: 'thing',\n",
       " 649: 'reference',\n",
       " 786: 'supposed',\n",
       " 173: 'copy',\n",
       " 61: 'away',\n",
       " 240: 'earlier',\n",
       " 51: 'arguments',\n",
       " 14: 'accept',\n",
       " 796: 'talking',\n",
       " 108: 'called',\n",
       " 697: 'science',\n",
       " 379: 'imagine',\n",
       " 832: 'try',\n",
       " 366: 'hold',\n",
       " 719: 'short',\n",
       " 698: 'scientific',\n",
       " 585: 'poor',\n",
       " 889: 'worse',\n",
       " 202: 'decided',\n",
       " 869: 'went',\n",
       " 710: 'sense',\n",
       " 888: 'world',\n",
       " 856: 'view',\n",
       " 437: 'little',\n",
       " 278: 'far',\n",
       " 411: 'known',\n",
       " 769: 'stay',\n",
       " 539: 'open',\n",
       " 795: 'talk',\n",
       " 142: 'claims',\n",
       " 70: 'basically',\n",
       " 468: 'making',\n",
       " 248: 'edu',\n",
       " 759: 'standard',\n",
       " 146: 'clock',\n",
       " 574: 'plan',\n",
       " 166: 'consider',\n",
       " 136: 'chips',\n",
       " 168: 'considering',\n",
       " 767: 'station',\n",
       " 565: 'pick',\n",
       " 752: 'sources',\n",
       " 820: 'today',\n",
       " 861: 'washington',\n",
       " 419: 'lead',\n",
       " 871: 'white',\n",
       " 81: 'bikes',\n",
       " 80: 'bike',\n",
       " 808: 'thank',\n",
       " 52: 'article',\n",
       " 693: 'saying',\n",
       " 283: 'feel',\n",
       " 398: 'isn',\n",
       " 833: 'trying',\n",
       " 623: 'quick',\n",
       " 230: 'doing',\n",
       " 806: 'tests',\n",
       " 819: 'times',\n",
       " 595: 'practice',\n",
       " 292: 'fit',\n",
       " 850: 'usually',\n",
       " 554: 'parts',\n",
       " 414: 'large',\n",
       " 534: 'office',\n",
       " 318: 'given',\n",
       " 415: 'late',\n",
       " 222: 'disease',\n",
       " 821: 'told',\n",
       " 825: 'town',\n",
       " 20: 'add',\n",
       " 295: 'folks',\n",
       " 407: 'kind',\n",
       " 880: 'wonder',\n",
       " 188: 'damage',\n",
       " 327: 'gordon',\n",
       " 174: 'correct',\n",
       " 766: 'states',\n",
       " 355: 'hello',\n",
       " 183: 'cross',\n",
       " 511: 'needed',\n",
       " 177: 'couldn',\n",
       " 65: 'banks',\n",
       " 504: 'n3jxp',\n",
       " 735: 'skepticism',\n",
       " 129: 'chastity',\n",
       " 391: 'intellect',\n",
       " 312: 'geb',\n",
       " 106: 'cadre',\n",
       " 239: 'dsl',\n",
       " 570: 'pitt',\n",
       " 718: 'shameful',\n",
       " 788: 'surrender',\n",
       " 499: 'months',\n",
       " 536: 'old',\n",
       " 273: 'fairly',\n",
       " 528: 'noticed',\n",
       " 738: 'small',\n",
       " 128: 'charge',\n",
       " 454: 'low',\n",
       " 154: 'commercial',\n",
       " 531: 'obvious',\n",
       " 875: 'windows',\n",
       " 113: 'cards',\n",
       " 497: 'monitor',\n",
       " 162: 'connect',\n",
       " 236: 'drivers',\n",
       " 520: 'night',\n",
       " 579: 'playing',\n",
       " 626: 'radio',\n",
       " 304: 'friend',\n",
       " 521: 'noise',\n",
       " 298: 'food',\n",
       " 427: 'levels',\n",
       " 372: 'human',\n",
       " 522: 'non',\n",
       " 21: 'addition',\n",
       " 812: 'things',\n",
       " 684: 'running',\n",
       " 282: 'fax',\n",
       " 459: 'machine',\n",
       " 441: 'logic',\n",
       " 498: 'month',\n",
       " 229: 'doesn',\n",
       " 302: 'forward',\n",
       " 491: 'minutes',\n",
       " 882: 'word',\n",
       " 287: 'file',\n",
       " 701: 'screen',\n",
       " 193: 'david',\n",
       " 665: 'required',\n",
       " 56: 'atheism',\n",
       " 296: 'follow',\n",
       " 516: 'news',\n",
       " 54: 'asked',\n",
       " 755: 'specific',\n",
       " 801: 'technology',\n",
       " 564: 'phone',\n",
       " 505: 'nasa',\n",
       " 667: 'research',\n",
       " 121: 'center',\n",
       " 329: 'gov',\n",
       " 410: 'knowledge',\n",
       " 748: 'sort',\n",
       " 679: 'room',\n",
       " 36: 'answers',\n",
       " 252: 'end',\n",
       " 178: 'count',\n",
       " 227: 'dod',\n",
       " 598: 'press',\n",
       " 672: 'return',\n",
       " 416: 'later',\n",
       " 433: 'limited',\n",
       " 66: 'base',\n",
       " 613: 'project',\n",
       " 176: 'costs',\n",
       " 264: 'expected',\n",
       " 417: 'launch',\n",
       " 589: 'possibly',\n",
       " 724: 'shuttle',\n",
       " 792: 'taken',\n",
       " 233: 'doubt',\n",
       " 831: 'true',\n",
       " 400: 'jim',\n",
       " 149: 'come',\n",
       " 221: 'discussion',\n",
       " 271: 'fact',\n",
       " 671: 'results',\n",
       " 123: 'certainly',\n",
       " 865: 'ways',\n",
       " 267: 'explain',\n",
       " 218: 'difficult',\n",
       " 112: 'card',\n",
       " 886: 'working',\n",
       " 457: 'lucky',\n",
       " 369: 'hope',\n",
       " 817: 'throw',\n",
       " 290: 'finding',\n",
       " 513: 'net',\n",
       " 45: 'apr',\n",
       " 435: 'lines',\n",
       " 733: 'situation',\n",
       " 211: 'device',\n",
       " 668: 'response',\n",
       " 449: 'loss',\n",
       " 306: 'function',\n",
       " 670: 'result',\n",
       " 96: 'bring',\n",
       " 226: 'doctor',\n",
       " 533: 'offer',\n",
       " 785: 'support',\n",
       " 643: 'recall',\n",
       " 156: 'companies',\n",
       " 708: 'sell',\n",
       " 662: 'report',\n",
       " 556: 'past',\n",
       " 535: 'oil',\n",
       " 683: 'run',\n",
       " 253: 'engine',\n",
       " 455: 'lower',\n",
       " 431: 'likely',\n",
       " 370: 'hot',\n",
       " 205: 'deleted',\n",
       " 841: 'unless',\n",
       " 60: 'avoid',\n",
       " 885: 'worked',\n",
       " 284: 'feet',\n",
       " 323: 'goes',\n",
       " 704: 'second',\n",
       " 551: 'paper',\n",
       " 40: 'apple',\n",
       " 530: 'numbers',\n",
       " 629: 'range',\n",
       " 805: 'test',\n",
       " 158: 'compare',\n",
       " 313: 'general',\n",
       " 109: 'came',\n",
       " 835: 'turned',\n",
       " 339: 'half',\n",
       " 630: 'rangers',\n",
       " 721: 'shouldn',\n",
       " 647: 'record',\n",
       " 739: 'smith',\n",
       " 152: 'comment',\n",
       " 225: 'division',\n",
       " 580: 'playoffs',\n",
       " 245: 'easy',\n",
       " 147: 'close',\n",
       " 254: 'entire',\n",
       " 703: 'season',\n",
       " 309: 'games',\n",
       " 763: 'starts',\n",
       " 204: 'definitely',\n",
       " 132: 'check',\n",
       " 200: 'decent',\n",
       " 289: 'finally',\n",
       " 631: 'rate',\n",
       " 203: 'defense',\n",
       " 71: 'basis',\n",
       " 424: 'left',\n",
       " 501: 'moon',\n",
       " 474: 'maybe',\n",
       " 87: 'bob',\n",
       " 382: 'included',\n",
       " 776: 'student',\n",
       " 102: 'business',\n",
       " 494: 'models',\n",
       " 238: 'driving',\n",
       " 673: 'ride',\n",
       " 137: 'choice',\n",
       " 493: 'model',\n",
       " 602: 'price',\n",
       " 153: 'comments',\n",
       " 92: 'boston',\n",
       " 8: '400',\n",
       " 7: '300',\n",
       " 500: 'montreal',\n",
       " 563: 'philadelphia',\n",
       " 5: '200',\n",
       " 567: 'piece',\n",
       " 728: 'simms',\n",
       " 586: 'port',\n",
       " 104: 'buying',\n",
       " 395: 'internal',\n",
       " 627: 'ram',\n",
       " 280: 'faster',\n",
       " 893: 'write',\n",
       " 220: 'directly',\n",
       " 259: 'example',\n",
       " 775: 'strong',\n",
       " 540: 'opinion',\n",
       " 587: 'position',\n",
       " 640: 'reasonable',\n",
       " 557: 'pay',\n",
       " 387: 'input',\n",
       " 219: 'digital',\n",
       " 161: 'computer',\n",
       " 376: 'ide',\n",
       " 100: 'built',\n",
       " 502: 'motherboard',\n",
       " 702: 'scsi',\n",
       " 265: 'expensive',\n",
       " 237: 'drives',\n",
       " 105: 'cable',\n",
       " 784: 'supply',\n",
       " 481: 'mention',\n",
       " 28: 'agree',\n",
       " 729: 'simple',\n",
       " 614: 'properly',\n",
       " 773: 'story',\n",
       " 730: 'simply',\n",
       " 24: 'advance',\n",
       " 32: 'alt',\n",
       " 824: 'total',\n",
       " 62: 'backup',\n",
       " 791: 'systems',\n",
       " 876: 'wings',\n",
       " 299: 'force',\n",
       " 101: 'bus',\n",
       " 550: 'pain',\n",
       " 636: 'realize',\n",
       " 342: 'happens',\n",
       " 584: 'points',\n",
       " 685: 'runs',\n",
       " 2: '1990',\n",
       " 249: 'effect',\n",
       " 38: 'apparently',\n",
       " 689: 'san',\n",
       " 592: 'posting',\n",
       " 883: 'words',\n",
       " 133: 'cheers',\n",
       " 261: 'exist',\n",
       " 361: 'higher',\n",
       " 235: 'driver',\n",
       " 285: 'field',\n",
       " 837: 'types',\n",
       " 413: 'lack',\n",
       " 180: 'couple',\n",
       " 644: 'received',\n",
       " 503: 'multi',\n",
       " 464: 'maintain',\n",
       " 794: 'taking',\n",
       " 770: 'steve',\n",
       " 803: 'term',\n",
       " 90: 'books',\n",
       " 686: 'safe',\n",
       " 257: 'evidence',\n",
       " 125: 'change',\n",
       " 537: 'older',\n",
       " 538: 'ones',\n",
       " 250: 'effective',\n",
       " 645: 'recent',\n",
       " 634: 'ready',\n",
       " 15: 'accepted',\n",
       " 761: 'started',\n",
       " 548: 'outside',\n",
       " 726: 'significant',\n",
       " 675: 'risk',\n",
       " 478: 'medicine',\n",
       " 190: 'data',\n",
       " 39: 'appears',\n",
       " 97: 'brown',\n",
       " 396: 'internet',\n",
       " 139: 'circuit',\n",
       " 600: 'prevent',\n",
       " 532: 'obviously',\n",
       " 223: 'disk',\n",
       " 678: 'rom',\n",
       " 91: 'boot',\n",
       " 617: 'pull',\n",
       " 422: 'learn',\n",
       " 485: 'michael',\n",
       " 99: 'build',\n",
       " 41: 'application',\n",
       " 790: 'switch',\n",
       " 212: 'devices',\n",
       " 135: 'chip',\n",
       " 754: 'special',\n",
       " 358: 'helps',\n",
       " 723: 'shows',\n",
       " 737: 'slow',\n",
       " 482: 'mentioned',\n",
       " 661: 'reply',\n",
       " 385: 'info',\n",
       " 603: 'prices',\n",
       " 877: 'winning',\n",
       " 143: 'class',\n",
       " 305: 'ftp',\n",
       " 732: 'site',\n",
       " 115: 'carry',\n",
       " 569: 'pins',\n",
       " 495: 'modem',\n",
       " 345: 'hardware',\n",
       " 757: 'spend',\n",
       " 568: 'pin',\n",
       " 141: 'claim',\n",
       " 597: 'present',\n",
       " 789: 'suspect',\n",
       " 609: 'produce',\n",
       " 547: 'output',\n",
       " 699: 'score',\n",
       " 405: 'key',\n",
       " 853: 'vehicle',\n",
       " 388: 'inside',\n",
       " 140: 'city',\n",
       " 31: 'allow',\n",
       " 16: 'access',\n",
       " 35: 'answer',\n",
       " 479: 'memory',\n",
       " 581: 'plug',\n",
       " 165: 'connector',\n",
       " 458: 'mac',\n",
       " 484: 'method',\n",
       " 346: 'hate',\n",
       " 151: 'coming',\n",
       " 375: 'ice',\n",
       " 638: 'rear',\n",
       " 272: 'fair',\n",
       " 244: 'east',\n",
       " 209: 'detroit',\n",
       " 807: 'texas',\n",
       " 134: 'chicago',\n",
       " 107: 'california',\n",
       " 713: 'series',\n",
       " 628: 'ran',\n",
       " 842: 'upgrade',\n",
       " 320: 'giving',\n",
       " 338: 'guys',\n",
       " 593: 'potential',\n",
       " 443: 'longer',\n",
       " 445: 'looked',\n",
       " 46: 'april',\n",
       " 461: 'magazine',\n",
       " 368: 'honda',\n",
       " 654: 'religion',\n",
       " 301: 'form',\n",
       " 383: 'includes',\n",
       " 873: 'willing',\n",
       " 650: 'regards',\n",
       " 517: 'newsgroup',\n",
       " 408: 'knew',\n",
       " 573: 'places',\n",
       " 29: 'ahead',\n",
       " 749: 'sound',\n",
       " 687: 'safety',\n",
       " 487: 'mike',\n",
       " 514: 'network',\n",
       " 847: 'uses',\n",
       " 460: 'machines',\n",
       " 830: 'trouble',\n",
       " 371: 'hours',\n",
       " 319: 'gives',\n",
       " 860: 'wants',\n",
       " 664: 'require',\n",
       " 11: '800',\n",
       " 362: 'highly',\n",
       " 722: 'shown',\n",
       " 744: 'somebody',\n",
       " 357: 'helpful',\n",
       " 43: 'appreciate',\n",
       " 826: 'traffic',\n",
       " 343: 'happy',\n",
       " 620: 'quality',\n",
       " 247: 'edge',\n",
       " 6: '2nd',\n",
       " 560: 'performance',\n",
       " 187: 'cut',\n",
       " 601: 'previous',\n",
       " 680: 'round',\n",
       " 490: 'minor',\n",
       " 268: 'external',\n",
       " 878: 'wish',\n",
       " 9: '486',\n",
       " 315: 'george',\n",
       " 432: 'limit',\n",
       " 163: 'connected',\n",
       " 439: 'load',\n",
       " 624: 'quickly',\n",
       " 353: 'heavy',\n",
       " 126: 'changed',\n",
       " 559: 'perfect',\n",
       " 131: 'cheaper',\n",
       " 260: 'excellent',\n",
       " 269: 'extra',\n",
       " 797: 'tank',\n",
       " 816: 'thread',\n",
       " 110: 'canada',\n",
       " 274: 'family',\n",
       " 619: 'putting',\n",
       " 700: 'scoring',\n",
       " 331: 'graphics',\n",
       " 582: 'plus',\n",
       " 127: 'changes',\n",
       " 840: 'university',\n",
       " 658: 'removed',\n",
       " 872: 'wide',\n",
       " 69: 'basic',\n",
       " 543: 'orbit',\n",
       " 72: 'bay',\n",
       " 157: 'company',\n",
       " 224: 'display',\n",
       " 677: 'robert',\n",
       " 404: 'keeping',\n",
       " 293: 'fixed',\n",
       " 706: 'seeing',\n",
       " 186: 'currently',\n",
       " 321: 'goal',\n",
       " 73: 'beat',\n",
       " 420: 'leafs',\n",
       " 542: 'option',\n",
       " 827: 'training',\n",
       " 868: 'weight',\n",
       " 294: 'floppy',\n",
       " 189: 'damn',\n",
       " 660: 'replacement'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map = dict((v, k) for k, v in vectorizer.vocabulary_.items()) \n",
    "id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Generate LDA model\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=6, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "Comparing to `corpora.Dictionary` use `id2word=id_map` instead of `id2word=dictionary`\n",
    "\n",
    "`ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20, random_state= 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review topics\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"edu\" + 0.019*\"com\" + 0.018*\"use\" + 0.018*\"thanks\" + 0.016*\"does\" + 0.015*\"know\" + 0.011*\"mail\" + 0.010*\"apple\" + 0.009*\"help\" + 0.008*\"want\"'),\n",
       " (1,\n",
       "  '0.061*\"drive\" + 0.039*\"disk\" + 0.030*\"scsi\" + 0.027*\"drives\" + 0.027*\"hard\" + 0.025*\"controller\" + 0.021*\"card\" + 0.018*\"rom\" + 0.016*\"cable\" + 0.016*\"floppy\"'),\n",
       " (2,\n",
       "  '0.024*\"people\" + 0.022*\"god\" + 0.012*\"atheism\" + 0.012*\"think\" + 0.012*\"believe\" + 0.012*\"don\" + 0.010*\"does\" + 0.010*\"just\" + 0.009*\"argument\" + 0.009*\"say\"'),\n",
       " (3,\n",
       "  '0.023*\"game\" + 0.021*\"year\" + 0.020*\"team\" + 0.013*\"games\" + 0.013*\"play\" + 0.011*\"good\" + 0.011*\"don\" + 0.010*\"think\" + 0.010*\"season\" + 0.010*\"players\"'),\n",
       " (4,\n",
       "  '0.035*\"space\" + 0.019*\"nasa\" + 0.018*\"data\" + 0.013*\"information\" + 0.013*\"available\" + 0.013*\"center\" + 0.011*\"ground\" + 0.010*\"research\" + 0.010*\"000\" + 0.010*\"new\"'),\n",
       " (5,\n",
       "  '0.017*\"just\" + 0.017*\"like\" + 0.016*\"don\" + 0.012*\"car\" + 0.012*\"time\" + 0.011*\"think\" + 0.011*\"good\" + 0.010*\"know\" + 0.008*\"way\" + 0.008*\"people\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=6,num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Name topics\n",
    "   \n",
    "</font>\n",
    "\n",
    "You need to name the topics manually (or use the top frequent words from topic )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_names= ['Education', 'Computers & IT', 'Religion', 'Sports', 'Science', 'Society & Lifestyle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Classify the new text \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's my understanding that the freezing will start to occur because of the\n",
      "growing distance of Pluto and Charon from the Sun, due to it's\n",
      "elliptical orbit. It is not due to shadowing effects. \n",
      "\n",
      "\n",
      "Pluto can shadow Charon, and vice-versa.\n",
      "\n",
      "George Krumins\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"] \n",
    "print (new_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.033417594),\n",
       "  (1, 0.03334091),\n",
       "  (2, 0.033516586),\n",
       "  (3, 0.033779573),\n",
       "  (4, 0.8323054),\n",
       "  (5, 0.033639915)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorized= vectorizer.transform(new_doc) # input param is list\n",
    "new_doc_corpus = gensim.matutils.Sparse2Corpus(doc_vectorized, documents_columns=False)\n",
    "doc_topics = ldamodel.get_document_topics(new_doc_corpus)\n",
    "list(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "def elicit_topic_name(doc_topics):    \n",
    "    return topics_names[np.squeeze(np.array(doc_topics))[:,1].argmax()]\n",
    "elicit_topic_name(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Topic Modeling \n",
    "\n",
    "</font>\n",
    "\n",
    "[voted-kaggle-dataset](https://www.kaggle.com/canggih/voted-kaggle-dataset/version/2#voted-kaggle-dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Versions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>Water Levels in Venezia, Italia</td>\n",
       "      <td>Venezia water levels from 1983 to 2015</td>\n",
       "      <td>Luis Bronchal</td>\n",
       "      <td>4</td>\n",
       "      <td>Version 2,2017-05-24|Version 1,2017-02-08</td>\n",
       "      <td>time series</td>\n",
       "      <td>Other</td>\n",
       "      <td>12 MB</td>\n",
       "      <td>CC0</td>\n",
       "      <td>1,363 views</td>\n",
       "      <td>85 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.kaggle.com/lbronchal/venezia</td>\n",
       "      <td>Context\\nThis dataset contains hourly measurem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Technology Price Index 2016</td>\n",
       "      <td>Where can you buy an iPhone cheaper?</td>\n",
       "      <td>Irina Kalatskaya</td>\n",
       "      <td>6</td>\n",
       "      <td>Version 1,2017-01-03</td>\n",
       "      <td>finance\\ntechnology forecasting</td>\n",
       "      <td>CSV</td>\n",
       "      <td>9 KB</td>\n",
       "      <td>Other</td>\n",
       "      <td>5,034 views</td>\n",
       "      <td>603 downloads</td>\n",
       "      <td>3 kernels</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.kaggle.com/ikalats/TechnologyPrice...</td>\n",
       "      <td>Context\\nLinio.com is Latin Americas leading ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>Investment growth forcast</td>\n",
       "      <td>growth of investment after some years?</td>\n",
       "      <td>Saqib Mujtaba</td>\n",
       "      <td>2</td>\n",
       "      <td>Version 1,2017-11-06</td>\n",
       "      <td>finance\\ninformation technology</td>\n",
       "      <td>CSV</td>\n",
       "      <td>645 B</td>\n",
       "      <td>CC0</td>\n",
       "      <td>356 views</td>\n",
       "      <td>42 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/saqibmujtaba/investment...</td>\n",
       "      <td>Description\\nThis is a data set for forecastin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title                                Subtitle  \\\n",
       "1522  Water Levels in Venezia, Italia  Venezia water levels from 1983 to 2015   \n",
       "1304      Technology Price Index 2016    Where can you buy an iPhone cheaper?   \n",
       "2008        Investment growth forcast  growth of investment after some years?   \n",
       "\n",
       "                 Owner  Votes                                   Versions  \\\n",
       "1522     Luis Bronchal      4  Version 2,2017-05-24|Version 1,2017-02-08   \n",
       "1304  Irina Kalatskaya      6                       Version 1,2017-01-03   \n",
       "2008     Saqib Mujtaba      2                       Version 1,2017-11-06   \n",
       "\n",
       "                                 Tags Data Type   Size License        Views  \\\n",
       "1522                      time series     Other  12 MB     CC0  1,363 views   \n",
       "1304  finance\\ntechnology forecasting       CSV   9 KB   Other  5,034 views   \n",
       "2008  finance\\ninformation technology       CSV  645 B     CC0    356 views   \n",
       "\n",
       "           Download    Kernels    Topics  \\\n",
       "1522   85 downloads        NaN       NaN   \n",
       "1304  603 downloads  3 kernels       NaN   \n",
       "2008   42 downloads        NaN  0 topics   \n",
       "\n",
       "                                                    URL  \\\n",
       "1522           https://www.kaggle.com/lbronchal/venezia   \n",
       "1304  https://www.kaggle.com/ikalats/TechnologyPrice...   \n",
       "2008  https://www.kaggle.com/saqibmujtaba/investment...   \n",
       "\n",
       "                                            Description  \n",
       "1522  Context\\nThis dataset contains hourly measurem...  \n",
       "1304  Context\\nLinio.com is Latin Americas leading ...  \n",
       "2008  Description\\nThis is a data set for forecastin...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "fn= 'voted-kaggle-dataset.csv'\n",
    "df = pd.read_csv(fn)\n",
    "print(len(df))\n",
    "df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts= 2,150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('len of texts= {:,}'.format(len(df)))\n",
    "index = 10 \n",
    "df.loc[index, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words='english', token_pattern='\\\\b\\\\w{3,}\\\\b')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.053*\"university\" + 0.010*\"state\" + 0.007*\"college\" + 0.005*\"data\" + 0.004*\"california\"'),\n",
       " (1,\n",
       "  '0.022*\"data\" + 0.017*\"dataset\" + 0.006*\"content\" + 0.006*\"context\" + 0.005*\"acknowledgements\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['Description'].values.astype('U')\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=1, \n",
    "    stop_words='english',\n",
    "    token_pattern=r\"\\b\\w{3,}\\b\") \n",
    "print(vectorizer.fit(text))\n",
    "\n",
    "vectorized = vectorizer.transform(text)\n",
    "corpus = gensim.matutils.Sparse2Corpus(vectorized, documents_columns=False)\n",
    "id_ = dict((i, j) for j, i in vectorizer.vocabulary_.items())\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=2, id2word=id_, passes=30, random_state=4381)\n",
    "\n",
    "ldamodel.print_topics(num_topics=2,num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "Latent Dirichlet allocation\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "LDA Algorithm Description.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Next lesson: Clustering \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
